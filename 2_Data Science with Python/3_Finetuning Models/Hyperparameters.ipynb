{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d05cf3c9",
   "metadata": {},
   "source": [
    "# Finetuning a Model - Hyperparameters\n",
    "Hyperparameters are parameters the data scientist can define that may improve model performance on test data. In general, we want to strike a balance between learning the patterns on the training data but not fitting exactly to the training data. In general, models tend to overfit the data as complexity of the model increases. Hyperparameters can be a way to use a complex model while avoiding overfitting the data.\n",
    "\n",
    "Try improving the decision tree classifier below to obtain an average accuracy score of 93% or higher. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f141e2c9",
   "metadata": {},
   "source": [
    "## Hyperparameter Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b7d345",
   "metadata": {},
   "source": [
    "Here are some of the best hyperparameters to start with in Decision Trees.\n",
    "\n",
    "**max_depth (default=None):** By default, there is no restriction to how deep the decision tree will be made. The deeper the tree, the more complex the model (which can cause overfitting on the train data). Try to define several max_depth to determine a balance between fit of training data and accuracy of classification for test data.  \n",
    "\n",
    "**min_samples_split (default=2):** Determines the number of samples a node needs before it is split. The smaller the min_samples_split, the more complex the model. Consider your sample size: does it make sense to have a minimum sample split of 2 if your are classifying 100,000 items? A useful starting place to test is 1% of sample size. \n",
    "\n",
    "**min_samples_leaf (default=1):** Determines the number of samples required to be a leaf. In other words, do enough samples classify in this way to be a pattern? By raising the min_samples_leaf paramter, you remove the potential for outliers to overfit on the training data. A useful starting place to test is 1% of sample size.\n",
    "\n",
    "**max_features (default=None):** Determines the number of features to consider *when splitting* a node. It's likely your model will perform best with a max_features of 2 or 3.  \n",
    "\n",
    "**max_leaf_nodes (default=None):** Determines number of total leaves in tree. As with max_depth, reducing the number of leaves can reduce complexity of the model to prevent overfitting. \n",
    "\n",
    "**class_weight (default=None):** The default is to assume classes occur at an equal rate. If we expect unbalanced classification, we can inversely weigh certain classes so that infrequent classifications are learned. As a simplified example, consider if our training data has 3 classes A:10,000, B:9,000, and C:500. We can either manually set weights that emphaize classifying C or by using class_weight='balanced', which would set weights based on the inverse of their frequency (e.g., C's weight would be (10,000+9,000+500)/500 = 39 versus A's weight of (10,000+9,000+500)/10,000 = 1.95). If you expect your classes are unbalanced, set class_weight='balanced' to compensate. \n",
    "\n",
    "*All hyperparameters for DecisionTreeClassifier() can be found here: https://scikit-learn.org/dev/modules/generated/sklearn.tree.DecisionTreeClassifier.html*\n",
    "\n",
    "Iteratively change a parameter one at a time to determine if the model is overfitting or underfitting the training data. Parameters will interact with one another! For example, if the tree is defined to be very shallow (max_depth=3) and the testing data is large, many leaves will be far larger than the min_samples_split size. While there are algorithms for finding optimal hyperparameters, our goal is to modify the model to create a reasonable increase in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06bb0fa",
   "metadata": {},
   "source": [
    "## Change Hyperparameters Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fdfcd2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Modify hyperparamters to create a refined decision tree classifier\n",
    "refined_clf_dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dece026a",
   "metadata": {},
   "source": [
    "## Run 10 Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4dc5ce59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 93%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load the wine dataset\n",
    "wine = load_wine()\n",
    "\n",
    "def run_clf_model(clf, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    refined_clf_dt.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    new_predictions = refined_clf_dt.predict(X_test)\n",
    "\n",
    "    # Evaluate new model accuracy\n",
    "    new_accuracy = accuracy_score(y_test, new_predictions)\n",
    "    return new_accuracy\n",
    "\n",
    "list_of_accuracies = []\n",
    "for index in range(0, 10):\n",
    "    model_accuracy = run_clf_model(refined_clf_dt, wine.data, wine.target)\n",
    "    list_of_accuracies.append(model_accuracy)\n",
    "\n",
    "print(f'Average Accuracy: {int(np.mean(list_of_accuracies)*100)}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
