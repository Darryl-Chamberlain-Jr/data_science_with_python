{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9670add5",
   "metadata": {},
   "source": [
    "# K-Means Cluster\n",
    "\n",
    "K-Means is a centroid-based clustering algorithm that partitions data into K clusters in an iterative manner. The \"means\" become the center of each cluster and the hope is the iteration of finding new means converges to a \"best\" center. Wikipedia has a great visualization for the general idea behind the iterative algorithm: https://en.wikipedia.org/wiki/K-means_clustering \n",
    "\n",
    "1. The initial \"means\" are randomly choosen within the data domain. \n",
    "2. Each data point is then categorized to the mean with shortest distance. \n",
    "3. A new mean is created as the average of all data points for each cluster. \n",
    "4. We forget the old data point categories and each data point is newly categorized to the new mean with shortest distance. \n",
    "5. We forget the old centroid and create the new centroid as the average of all data points for each cluster.\n",
    "5. We continue the loop of creating new centroids and new clusters of data points until the clusters produce minimal to no change. \n",
    "\n",
    "<a title=\"Chire, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:K-means_convergence.gif\"><img width=\"512\" alt=\"K-means convergence\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/K-means_convergence.gif/512px-K-means_convergence.gif?20170530143526\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae70d068",
   "metadata": {},
   "source": [
    "Notice that our initial \"means\" for red and yellow are very off. That's okay! The iterative process moves these random guesses toward the clusters of data. In fact, Python uses a \"greedy\" algorithm to speed up the iterations, but that isn't important to know in order to make your first K-means clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33213c4",
   "metadata": {},
   "source": [
    "Let's see how to implement a K-means cluster in Python using random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49c98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Generate random data to demonstrate clustering\n",
    "data = np.random.rand(100, 2)\n",
    "\n",
    "# Be sure to specify number of clusters!\n",
    "clf_kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "# Fit the model to the data\n",
    "# Note this is the final iteration result - we won't know about each iteration\n",
    "clf_kmeans.fit(data)\n",
    "\n",
    "# import packages for graphing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# labels are the classification names for each cluster\n",
    "labels = clf_kmeans.labels_\n",
    "\n",
    "# centroids are the centers of the clusters for each label\n",
    "centroids = clf_kmeans.cluster_centers_\n",
    "\n",
    "# Plot the data points and centroids\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', alpha=0.7, edgecolors='k')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200)\n",
    "plt.title('K-Means Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae24880a",
   "metadata": {},
   "source": [
    "As you can see, we can always make a clustering but it may not be clear that data *actually* seems to cluster around the centroids. Let's see what happens when we apply it to some of the SKLearn datasets where we can consider the accuracy of the clustering for predicting future classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62297b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def create_2_feature_wine_data(feature_1, feature_2):\n",
    "    # Load the wine dataset\n",
    "    wine = load_wine()\n",
    "    wine_data_2_features = np.column_stack((wine.data[:, feature_1], wine.data[:, feature_2]))\n",
    "    return [wine, wine_data_2_features]\n",
    "\n",
    "def run_kmeans_wine(wine, wine_data_2_features):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(wine_data_2_features, wine.target, test_size=0.2)\n",
    "\n",
    "    # Create a decision tree classifier\n",
    "    clf_kmeans_wine = KMeans(n_clusters=3)\n",
    "    clf_kmeans_wine.fit(X_train, y_train)\n",
    "\n",
    "    return [clf_kmeans_wine, X_train, X_test, y_train, y_test]\n",
    "\n",
    "def graph_kmeans_wine(wine, clf_kmeans_wine, X_train, X_test, y_train, y_test, feature_1, feature_2):\n",
    "    # Evaluate accuracy\n",
    "    predictions = clf_kmeans_wine.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print_result = f'{int(round(accuracy, 2)*100)}%'\n",
    "\n",
    "    # labels are the classification names for each cluster\n",
    "    labels_wine_train = clf_kmeans_wine.labels_\n",
    "    labels_wine_test = y_test\n",
    "\n",
    "    # centroids are the centers of the clusters for each label\n",
    "    centroids_wine = clf_kmeans_wine.cluster_centers_\n",
    "\n",
    "    # Plot the data points and centroids\n",
    "    plt.scatter(X_train[:, 0], X_train[:, 1], c=labels_wine_train, cmap='viridis', alpha=0.7, edgecolors='k')\n",
    "    plt.scatter(X_test[:, 0], X_test[:, 1], c=labels_wine_test, cmap='viridis', alpha=0.3, edgecolors='k')\n",
    "    plt.scatter(centroids_wine[:, 0], centroids_wine[:, 1], c='red', marker='X', s=200)\n",
    "    plt.title(f'K-Means 2-feature Clustering Accuracy: {print_result}')\n",
    "    plt.xlabel(f'{wine.feature_names[feature_1]}')\n",
    "    plt.ylabel(f'{wine.feature_names[feature_2]}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fb53b",
   "metadata": {},
   "source": [
    "## 2-Means Cluster\n",
    "\n",
    "We know the wine dataset has excellent accuracy when using all 13 features, but we can't visualize a 13 dimensional space. Choose 2 features to create a 2d clustering for classification.\n",
    "- 'alcohol': 0,\n",
    "- 'malic_acid': 1,\n",
    "- 'ash': 2,\n",
    "- 'alcalinity_of_ash': 3,\n",
    "- 'magnesium': 4,\n",
    "- 'total_phenols': 5,\n",
    "- 'flavanoids': 6,\n",
    "- 'nonflavanoid_phenols': 7,\n",
    "- 'proanthocyanins': 8,\n",
    "- 'color_intensity': 9,\n",
    "- 'hue': 10,\n",
    "- 'od280/od315_of_diluted_wines': 11,\n",
    "- 'proline': 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62be7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these to graph different features!\n",
    "feature_1 = 0\n",
    "feature_2 = 1\n",
    "\n",
    "wine, wine_data_2_features = create_2_feature_wine_data(feature_1, feature_2)\n",
    "clf_kmeans_wine, X_train, X_test, y_train, y_test = run_kmeans_wine(wine, wine_data_2_features)\n",
    "graph_kmeans_wine(wine, clf_kmeans_wine, X_train, X_test, y_train, y_test, feature_1, feature_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acb2cd1",
   "metadata": {},
   "source": [
    "The darker circles represent our training data and the lighter circles represent our test data. Ideally, we'd want the nearby data for the training and test data to match color to signify they are being classified in the same way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276ebc00",
   "metadata": {},
   "source": [
    "## 2-Means vs 3-Means Clustering\n",
    "Let's try to add 1 more feature and compare the 2-feature and 3-feature clusterings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11936fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2_and_3_feature_data(feature_1, feature_2, feature_3):\n",
    "    # Load the wine dataset\n",
    "    wine = load_wine()\n",
    "    wine_data_2_features = np.column_stack((wine.data[:, feature_1], wine.data[:, feature_2]))\n",
    "    wine_data_3_features = np.column_stack((wine.data[:, feature_1], wine.data[:, feature_2], wine.data[:, feature_3]))\n",
    "    return [wine_data_2_features, wine_data_3_features]\n",
    "\n",
    "def run_kmeans_3_wine(wine, wine_data_3_features):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(wine_data_3_features, wine.target, test_size=0.2)\n",
    "\n",
    "    # Create a decision tree classifier\n",
    "    clf_kmeans_wine = KMeans(n_clusters=3)\n",
    "    clf_kmeans_wine.fit(X_train, y_train)\n",
    "\n",
    "    return [clf_kmeans_wine, X_train, X_test, y_train, y_test]\n",
    "\n",
    "def graph_kmeans_3_wine(wine, clf_kmeans_wine, X_train, X_test, y_train, y_test, feature_1, feature_2, feature_3):\n",
    "    # Evaluate accuracy\n",
    "    predictions_3 = clf_kmeans_wine.predict(X_test)\n",
    "    accuracy_3 = accuracy_score(y_test, predictions_3)\n",
    "    print_result_3 = f'{int(round(accuracy_3, 2)*100)}%'\n",
    "\n",
    "    # labels are the classification names for each cluster\n",
    "    labels_wine_train = clf_kmeans_wine.labels_\n",
    "    labels_wine_test = y_test\n",
    "\n",
    "    # centroids are the centers of the clusters for each label\n",
    "    centroids_wine = clf_kmeans_wine.cluster_centers_\n",
    "\n",
    "    # Plot the data points and centroids\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Note the shift from plotting on ax\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], X_train[:, 2], c=labels_wine_train, cmap='viridis', alpha=0.7, edgecolors='k')\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c=labels_wine_test, cmap='viridis', alpha=0.3, edgecolors='k')\n",
    "    ax.scatter(centroids_wine[:, 0], centroids_wine[:, 1], centroids_wine[:, 2], c='red', marker='X', s=200)\n",
    "    \n",
    "    ax.set_xlabel(f'{wine.feature_names[feature_1]}')\n",
    "    ax.set_ylabel(f'{wine.feature_names[feature_2]}')\n",
    "    ax.set_zlabel(f'{wine.feature_names[feature_3]}')\n",
    "\n",
    "    plt.title(f'K-Means 3-feature Clustering Accuracy: {print_result_3}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4283192e",
   "metadata": {},
   "source": [
    "Phew! At least we have an example of 3d plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a243b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these to graph different features!\n",
    "feature_1 = 0\n",
    "feature_2 = 1\n",
    "feature_3 = 2\n",
    "\n",
    "wine_data_2_features, wine_data_3_features = create_2_and_3_feature_data(feature_1, feature_2, feature_3)\n",
    "\n",
    "clf_kmeans_wine, X_train, X_test, y_train, y_test = run_kmeans_wine(wine, wine_data_2_features)\n",
    "graph_kmeans_wine(wine, clf_kmeans_wine, X_train, X_test, y_train, y_test, feature_1, feature_2)\n",
    "\n",
    "clf_kmeans_wine_3, X_train_3, X_test_3, y_train_3, y_test_3 = run_kmeans_3_wine(wine, wine_data_3_features)\n",
    "graph_kmeans_3_wine(wine, clf_kmeans_wine_3, X_train_3, X_test_3, y_train_3, y_test_3, feature_1, feature_2, feature_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb700f7f",
   "metadata": {},
   "source": [
    "While adding a feature usually increases the accuracy, it may not always increase the accuracy. There are techniques for figuring out which features are most important which will be presented in future lessons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
